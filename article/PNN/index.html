<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">


<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"hu1104.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>


  <meta name="description" content="1.动机 2. 模型的结构及原理 2.1 Product Layer 2.1.1 IPNN     3.代码实现    1.动机在特征交叉的相关模型中FM, FFM都证明了特征交叉的重要性，DNN将神经网络的高阶隐式交叉加到了FM的二阶特征交叉上，一定程度上说明了DNN做特征交叉的有效性。但是对于DNN这种“add”操作的特征交叉并不能充分挖掘类别特征的交叉效果。PNN虽然也用了DNN来">
<meta property="og:type" content="article">
<meta property="og:title" content="PNN">
<meta property="og:url" content="https://hu1104.github.io/article/PNN/index.html">
<meta property="og:site_name" content="Zongxing">
<meta property="og:description" content="1.动机 2. 模型的结构及原理 2.1 Product Layer 2.1.1 IPNN     3.代码实现    1.动机在特征交叉的相关模型中FM, FFM都证明了特征交叉的重要性，DNN将神经网络的高阶隐式交叉加到了FM的二阶特征交叉上，一定程度上说明了DNN做特征交叉的有效性。但是对于DNN这种“add”操作的特征交叉并不能充分挖掘类别特征的交叉效果。PNN虽然也用了DNN来">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://hu1104.github.io/article/PNN/PNN/PNN%E7%BB%93%E6%9E%84.png">
<meta property="og:image" content="https://hu1104.github.io/article/PNN/PNN/product%E5%B1%82.png">
<meta property="article:published_time" content="2021-10-27T00:55:13.000Z">
<meta property="article:modified_time" content="2021-10-27T07:17:20.395Z">
<meta property="article:author" content="hzx">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://hu1104.github.io/article/PNN/PNN/PNN%E7%BB%93%E6%9E%84.png">

<link rel="canonical" href="https://hu1104.github.io/article/PNN/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>PNN | Zongxing</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

  <!-- <script type="text/javascript" src="/js/love.js"></script> -->
   <!-- 爆炸红心效果 -->
<!--<canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas> 
<script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script> 
<script type="text/javascript" src="/js/firework.js"></script>-->

<link rel="alternate" href="/atom.xml" title="Zongxing" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Zongxing</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">你若安好，便是晴天！</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签<span class="badge">3</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>分类<span class="badge">1</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档<span class="badge">41</span></a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="sitemap fa-fw"></i>站点地图</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://hu1104.github.io/article/PNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="hzx">
      <meta itemprop="description" content="代码千万行，注释第一行。</br>编程不规范，同事两行泪。">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zongxing">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          PNN
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-10-27 08:55:13" itemprop="dateCreated datePublished" datetime="2021-10-27T08:55:13+08:00">2021-10-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-10-27 15:17:20" itemprop="dateModified" datetime="2021-10-27T15:17:20+08:00">2021-10-27</time>
              </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>7.2k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>7 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

<ul>
<li><a href="#1%E5%8A%A8%E6%9C%BA">1.动机</a></li>
<li><a href="#2-%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%93%E6%9E%84%E5%8F%8A%E5%8E%9F%E7%90%86">2. 模型的结构及原理</a><ul>
<li><a href="#21-product-layer">2.1 Product Layer</a><ul>
<li><a href="#211-ipnn">2.1.1 IPNN</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#3%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0">3.代码实现</a></li>
</ul>
<!-- /code_chunk_output -->

<h3 id="1动机"><a href="#1-动机" class="headerlink" title="1.动机"></a>1.动机<a href="#1动机" class="header-anchor">#</a></h3><p>在特征交叉的相关模型中FM, FFM都证明了特征交叉的重要性，DNN将神经网络的高阶隐式交叉加到了FM的二阶特征交叉上，一定程度上说明了DNN做特征交叉的有效性。但是对于DNN这种“add”操作的特征交叉并不能充分挖掘类别特征的交叉效果。PNN虽然也用了DNN来对特征进行交叉组合，但是并不是直接将低阶特征放入DNN中，而是设计了Product层先对低阶特征进行充分的交叉组合之后再送入到DNN中去。</p>
<p>PNN模型其实是对IPNN和OPNN的总称，两者分别对应的是不同的Product实现方法，前者采用的是inner product，后者采用的是outer product。在PNN的算法方面，比较重要的部分就是Product Layer的简化实现方法，需要在数学和代码上都能够比较深入的理解。</p>
<span id="more"></span>
<h3 id="2-模型的结构及原理"><a href="#2-模型的结构及原理" class="headerlink" title="2. 模型的结构及原理"></a>2. 模型的结构及原理<a href="#2-模型的结构及原理" class="header-anchor">#</a></h3><p>PNN模型的整体架构如下图所示：<br><img src="./PNN/PNN%E7%BB%93%E6%9E%84.png" alt="PNN模型"></p>
<p>Product层主要有线性部分和非线性部分组成，分别用$l_z$和$l_p$来表示，<br><img src="PNN/product%E5%B1%82.png" alt="product层"></p>
<ol>
<li>线性模块，一阶特征(未经过显示特征交叉处理)，对应论文中的$l_z=(l_z^1,l_z^2, …, l_z^{D_1})$</li>
<li>非线性模块，高阶特征(经过显示特征交叉处理)，对应论文中的$l_p=(l_p^1,l_p^2, …, l_p^{D_1})$<h4 id="21-product-layer"><a href="#2-1-Product-Layer" class="headerlink" title="2.1 Product Layer"></a>2.1 Product Layer<a href="#21-product-layer" class="header-anchor">#</a></h4></li>
</ol>
<p>==线性部分==</p>
<p>先来解释一下$l_z$是如何计算得到的，在介绍计算$l_z$之前先介绍一下矩阵内积计算, 如下公式所示，用一句话来描述就是两个矩阵对应元素相称，然后将相乘之后的所有元素相加 $$ A \odot{B} = \sum_{i,j}A_{i,j}B_{i,j} $$ $l_z^n$的计算就是矩阵内积，而$l_z$是有$D_1$个$l_z^n$组成，所以需要$D1$个矩阵求得，但是在代码实现的时候不一定是定义$D_1$个矩阵，可以将这些矩阵Flatten，具体的细节可以参考给出的代码。 $$ l_z=(l_z^1,l_z^2, …, l_z^{D_1})\ l_z^n = W_z^n \odot{z} \ z = (z_1, z_2, …, z_N) $$ 总之这一波操作就是将所有的embedding向量中的所有元素都乘以一个矩阵的对应元素，最后相加即可，这一部分比较简单(N表示的是特征的数量，M表示的是所有特征转化为embedding之后维度，也就是N*emb_dim) $$ l_z^n = W_z^n \odot{z} = \sum_{i=1}^N \sum_{j=1}^M (W_z^n)<em>{i,j}z</em>{i,j} $$</p>
<p>==非线性部分==</p>
<p>上面介绍了线性部分$l_p$的计算，非线性部分的计算相比线性部分要复杂很多，先从整体上看$l_p$的计算 $$ l_p=(l_p^1,l_p^2, …, l_p^{D_1}) \ l_p^n = W_p^n \odot{p} \ p = {p_{i,j}}, i=1,2,…,N,j=1,2,…,N $$ 从上述公式中可以发现，$l_p^n$和$l_z^n$类似需要$D_1$个$W_p^n$矩阵计算内积得到，重点就是如何求这个$p$，这里作者提出了两种方式，一种是使用内积计算，另一种是使用外积计算。</p>
<h5 id="211-ipnn"><a href="#2-1-1-IPNN" class="headerlink" title="2.1.1 IPNN"></a>2.1.1 IPNN<a href="#211-ipnn" class="header-anchor">#</a></h5><p>使用内积实现特征交叉就和FM是类似的(两两向量计算内积)，下面将向量内积操作表示如下表达式 $$ g(f_i,f_j) = &lt;f_i, f_j&gt; $$ 将内积的表达式带入$l_p^n$的计算表达式中有： $$ \begin{align}<br>l_p^n &amp;= W_p^n \odot{p} \<br> &amp;= \sum_{i=1}^N \sum_{j=1}^N (W_p^n)<em>{i,j}p</em>{i,j} \<br>  &amp;= \sum_{i=1}^N \sum_{j=1}^N (W_p^n)<em>{i,j}&lt;f_i, f_j&gt;\end{align} $$<br>上面就提到了这里使用的内积是计算两两特征之间的内积，然而向量a和向量b的内积与向量b和向量a的内积是相同的，其实是没必要计算的，看一下下面FM的计算公式： $$ \hat{y}(X) = \omega</em>{0}+\sum_{i=1}^{n}{\omega_{i}x_{i}}+\sum_{i=1}^{n}{\sum_{j=i+1}^{n} &lt;v_{i},v_{j}&gt;x_{i}x_{j}} $$ 也就是说计算的内积矩阵$p$是对称的，那么与其对应元素做矩阵内积的矩阵$W_p^n$也是对称的，对于可学习的权重来说如果是对称的是不是可以只使用其中的一半就行了呢，所以基于这个思考，对Inner Product的权重定义及内积计算进行优化，首先将权重矩阵分解$W_p^n=\theta^n \theta^{nT}$,此时$\theta^n \in R^N$（参数从原来的$N^2$变成了$N$）,将分解后的$W_p^n$带入$l_p^n$的计算公式有： $$ \begin{align}<br>l_p^n &amp;= W_p^n \odot{p} \<br> &amp;= \sum_{i=1}^N \sum_{j=1}^N (W_p^n){i,j}p{i,j} \<br>  &amp;= \sum_{i=1}^N \sum_{j=1}^N \theta^n \theta^n &lt;f_i, f_j&gt; \<br>   &amp;= \sum_{i=1}^N \sum_{j=1}^N &lt;\theta^n f_i, \theta^n f_j&gt; \<br>    &amp;= &lt;\sum_{i=1}^N \theta^n f_i, \sum_{j=1}^N \theta^n f_j&gt; \<br>     &amp;= ||\sum_{i=1}^N \theta^n f_i||^2 \end{align} $$<br>所以优化后的$l_p$的计算公式为： $$ l_p = (||\sum_{i=1}^N \theta^1 f_i||^2, ||\sum_{i=1}^N \theta^2 f_i||^2, …, ||\sum_{i=1}^N \theta^{D_1} f_i||^2) $$ 这里为了好理解不做过多的解释，其实这里对于矩阵分解省略了一些细节，感兴趣的可以去看原文，最后模型实现的时候就是基于上面的这个公式计算的（给出的代码也是基于优化之后的实现）。</p>
<p>#####2.1.2 OPNN<br>使用外积实现相比于使用内积实现，唯一的区别就是使用向量的外积来计算矩阵$p$,首先定义向量的外积计算 $$ g(i,j) = f_i f_j^T $$ 从外积公式可以发现两个向量的外积得到的是一个矩阵，与上面介绍的内积计算不太相同，内积得到的是一个数值。内积实现的Product层是将计算得到的内积矩阵，乘以一个与其大小一样的权重矩阵，然后求和，按照这个思路的话，通过外积得到的$p$计算$W_p^n \odot{p}$相当于之前的内积值乘以权重矩阵对应位置的值求和就变成了，外积矩阵乘以权重矩阵中对应位置的子矩阵然后将整个相乘得到的大矩阵对应元素相加，用公式表示如下： $$ \begin{align}l_p^n &amp;= W_p^n \odot{p} \<br> &amp;= \sum_{i=1}^N \sum_{j=1}^N (W_p^n){i,j}p{i,j} \<br>  &amp;= \sum_{i=1}^N \sum_{j=1}^N (W_p^n)<em>{i,j} f_i f_j^T<br>\end{align} $$<br>需要注意的是此时的$(W_p^n){i,j}$表示的是一个矩阵，而不是一个值，此时计算$l_p$的复杂度是$O(D_1N^2M^2)$, 其中$N^2$表示的是特征的组合数量，$M^2$表示的是计算外积的复杂度。这样的复杂度肯定是无法接受的，所以为了优化复杂度，PNN的作者重新定义了$p$的计算方式：<br>$$ p=\sum{i=1}^N \sum</em>{j=1}^N f_i f_j^T = f_{\sum}(f_{\sum})^T\ f_{\sum} = \sum_{i=1}^N f_i $$<br>需要注意，这里新定义的外积计算与传统的外积计算时不等价的，这里是为了优化计算效率重新定义的计算方式，从公式中可以看出，相当于先将原来的embedding向量在特征维度上先求和，变成一个向量之后再计算外积。加入原embedding向量表示为$E \in R^{N\times M}$，其中$N$表示特征的数量，M表示的是所有特征的总维度，即$Nemb_dim$, 在特征维度上进行求和就是将$E \in R^{N\times M}$矩阵压缩成了$E \in R^M$, 然后两个$M$维的向量计算外积得到最终所有特征的外积交叉结果$p\in R^{M\times M}$，最终的$l_p^n$可以表示为： $$ l_p^n = W_p^n \odot{p} = \sum_{i=1}^N \sum_{j=1}^N (W_p^n)<em>{i,j}p</em>{i,j}  $$ 最终的计算方式和$l_z$的计算方式看起来差不多，但是需要注意外积优化后的$W_p^n$的维度是$R^{M \times M}$的，$M$表示的是特征矩阵的维度，即$N_{emb_dim}$。</p>
<h3 id="3代码实现"><a href="#3-代码实现" class="headerlink" title="3.代码实现"></a>3.代码实现<a href="#3代码实现" class="header-anchor">#</a></h3><p>代码实现的整体逻辑比较简单，就是对类别特征进行embedding编码，然后通过embedding特征计算$l_z,l_p$, 接着将$l_z, l_p$的输出concat到一起输入到DNN中得到最终的预测结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def PNN(dnn_feature_columns, inner=True, outer=True):</span><br><span class="line">    # 构建输入层，即所有特征对应的Input()层，这里使用字典的形式返回，方便后续构建模型</span><br><span class="line">    _, sparse_input_dict = build_input_layers(dnn_feature_columns)</span><br><span class="line"></span><br><span class="line">    # 构建模型的输入层，模型的输入层不能是字典的形式，应该将字典的形式转换成列表的形式</span><br><span class="line">    # 注意：这里实际的输入与Input()层的对应，是通过模型输入时候的字典数据的key与对应name的Input层</span><br><span class="line">    input_layers = list(sparse_input_dict.values())</span><br><span class="line">    </span><br><span class="line">    # 构建维度为k的embedding层，这里使用字典的形式返回，方便后面搭建模型</span><br><span class="line">    embedding_layer_dict = build_embedding_layers(dnn_feature_columns, sparse_input_dict, is_linear=False)</span><br><span class="line"></span><br><span class="line">    sparse_embed_list = concat_embedding_list(dnn_feature_columns, sparse_input_dict, embedding_layer_dict, flatten=False)</span><br><span class="line"></span><br><span class="line">    dnn_inputs = ProductLayer(units=32, use_inner=True, use_outer=True)(sparse_embed_list)</span><br><span class="line">    </span><br><span class="line">    # 输入到dnn中，需要提前定义需要几个残差块</span><br><span class="line">    output_layer = get_dnn_logits(dnn_inputs)</span><br><span class="line"></span><br><span class="line">    model = Model(input_layers, output_layer)</span><br><span class="line">    return model</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>PNN的难点就是Product层的实现，下面是Product 层实现的代码，代码中是使用优化之后$l_p$的计算方式编写的, 代码中有详细的注释，但是要完全理解代码还需要去理解上述说过的优化思路。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">class ProductLayer(Layer):</span><br><span class="line">    def __init__(self, units, use_inner=True, use_outer=False):</span><br><span class="line">        super(ProductLayer, self).__init__()</span><br><span class="line">        self.use_inner = use_inner</span><br><span class="line">        self.use_outer = use_outer</span><br><span class="line">        self.units = units # 指的是原文中D1的大小</span><br><span class="line"></span><br><span class="line">    def build(self, input_shape):</span><br><span class="line">        # 需要注意input_shape也是一个列表，并且里面的每一个元素都是TensorShape类型，</span><br><span class="line">        # 需要将其转换成list然后才能参与数值计算，不然类型容易错</span><br><span class="line">        # input_shape[0] : feat_nums x embed_dims</span><br><span class="line">        self.feat_nums = len(input_shape)</span><br><span class="line">        self.embed_dims = input_shape[0].as_list()[-1]</span><br><span class="line">        flatten_dims = self.feat_nums * self.embed_dims</span><br><span class="line"></span><br><span class="line">        # Linear signals weight, 这部分是用于产生Z的权重，因为这里需要计算的是两个元素对应元素乘积然后再相加</span><br><span class="line">        # 等价于先把矩阵拉成一维，然后相乘再相加</span><br><span class="line">        self.linear_w = self.add_weight(name=&#x27;linear_w&#x27;, shape=(flatten_dims, self.units), initializer=&#x27;glorot_normal&#x27;)</span><br><span class="line"></span><br><span class="line">        # inner product weight</span><br><span class="line">        if self.use_inner:</span><br><span class="line">            # 优化之后的内积权重是未优化时的一个分解矩阵，未优化时的矩阵大小为：D x N x N </span><br><span class="line">            # 优化后的内积权重大小为：D x N</span><br><span class="line">            self.inner_w = self.add_weight(name=&#x27;inner_w&#x27;, shape=(self.units, self.feat_nums), initializer=&#x27;glorot_normal&#x27;)</span><br><span class="line"></span><br><span class="line">        if self.use_outer:</span><br><span class="line">            # 优化之后的外积权重大小为：D x embed_dim x embed_dim, 因为计算外积的时候在特征维度通过求和的方式进行了压缩</span><br><span class="line">            self.outer_w = self.add_weight(name=&#x27;outer_w&#x27;, shape=(self.units, self.embed_dims, self.embed_dims), initializer=&#x27;glorot_normal&#x27;)</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    def call(self, inputs):</span><br><span class="line">        # inputs是一个列表</span><br><span class="line">        # 先将所有的embedding拼接起来计算线性信号部分的输出</span><br><span class="line">        concat_embed = Concatenate(axis=1)(inputs) # B x feat_nums x embed_dims</span><br><span class="line">        # 将两个矩阵都拉成二维的，然后通过矩阵相乘得到最终的结果</span><br><span class="line">        concat_embed_ = tf.reshape(concat_embed, shape=[-1, self.feat_nums * self.embed_dims])</span><br><span class="line">        lz = tf.matmul(concat_embed_, self.linear_w) # B x units</span><br><span class="line"></span><br><span class="line">        # inner</span><br><span class="line">        lp_list = []</span><br><span class="line">        if self.use_inner:</span><br><span class="line">            for i in range(self.units):</span><br><span class="line">                # 相当于给每一个特征向量都乘以一个权重</span><br><span class="line">                # self.inner_w[i] : (embed_dims, ) 添加一个维度变成 (embed_dims, 1)</span><br><span class="line">                delta = tf.multiply(concat_embed, tf.expand_dims(self.inner_w[i], axis=1)) # B x feat_nums x embed_dims</span><br><span class="line">                # 在特征之间的维度上求和</span><br><span class="line">                delta = tf.reduce_sum(delta, axis=1) # B x embed_dims</span><br><span class="line">                # 最终在特征embedding维度上求二范数得到p</span><br><span class="line">                lp_list.append(tf.reduce_sum(tf.square(delta), axis=1, keepdims=True)) # B x 1</span><br><span class="line">            </span><br><span class="line">        # outer</span><br><span class="line">        if self.use_outer:</span><br><span class="line">            # 外积的优化是将embedding矩阵，在特征间的维度上通过求和进行压缩</span><br><span class="line">            feat_sum = tf.reduce_sum(concat_embed, axis=1) # B x embed_dims</span><br><span class="line">            </span><br><span class="line">            # 为了方便计算外积，将维度进行扩展</span><br><span class="line">            f1 = tf.expand_dims(feat_sum, axis=2) # B x embed_dims x 1</span><br><span class="line">            f2 = tf.expand_dims(feat_sum, axis=1) # B x 1 x embed_dims</span><br><span class="line"></span><br><span class="line">            # 求外积, a * a^T</span><br><span class="line">            product = tf.matmul(f1, f2) # B x embed_dims x embed_dims</span><br><span class="line"></span><br><span class="line">            # 将product与外积权重矩阵对应元素相乘再相加</span><br><span class="line">            for i in range(self.units):</span><br><span class="line">                lpi = tf.multiply(product, self.outer_w[i]) # B x embed_dims x embed_dims</span><br><span class="line">                # 将后面两个维度进行求和，需要注意的是，每使用一次reduce_sum就会减少一个维度</span><br><span class="line">                lpi = tf.reduce_sum(lpi, axis=[1, 2]) # B</span><br><span class="line">                # 添加一个维度便于特征拼接</span><br><span class="line">                lpi = tf.expand_dims(lpi, axis=1) # B x 1</span><br><span class="line">                lp_list.append(lpi)</span><br><span class="line">            </span><br><span class="line">        # 将所有交叉特征拼接到一起</span><br><span class="line">        lp = Concatenate(axis=1)(lp_list)</span><br><span class="line"></span><br><span class="line">        # 将lz和lp拼接到一起</span><br><span class="line">        product_out =  Concatenate(axis=1)([lz, lp])</span><br><span class="line">        </span><br><span class="line">        return product_out</span><br><span class="line"></span><br></pre></td></tr></table></figure>
    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/article/deepcrossing/" rel="prev" title="deepcrossing">
      <i class="fa fa-chevron-left"></i> deepcrossing
    </a></div>
      <div class="post-nav-item">
    <a href="/article/Wide-deep/" rel="next" title="Wide_deep">
      Wide_deep <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#1%E5%8A%A8%E6%9C%BA"><span class="nav-number">1.</span> <span class="nav-text">1.动机</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%93%E6%9E%84%E5%8F%8A%E5%8E%9F%E7%90%86"><span class="nav-number">2.</span> <span class="nav-text">2. 模型的结构及原理</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#21-product-layer"><span class="nav-number">2.1.</span> <span class="nav-text">2.1 Product Layer</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#211-ipnn"><span class="nav-number">2.1.1.</span> <span class="nav-text">2.1.1 IPNN</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="nav-number">3.</span> <span class="nav-text">3.代码实现</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">hzx</p>
  <div class="site-description" itemprop="description">代码千万行，注释第一行。</br>编程不规范，同事两行泪。</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">41</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">hzx</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">224k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">3:23</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

  
  <canvas class="fireworks" style="position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;" ></canvas>
  <script type="text/javascript" src="//cdn.bootcss.com/animejs/2.2.0/anime.min.js"></script>
  <script type="text/javascript" src="/js/fireworks.js"></script>
  
<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"log":false,"model":{"scale":1,"hHeadPos":0.5,"vHeadPos":0.618,"jsonPath":"/live2dw/assets/haruto.model.json"},"display":{"superSample":2,"width":125,"height":125,"position":"left","hOffset":30,"vOffset":20},"mobile":{"show":true,"scale":0.05},"react":{"opacityDefault":1,"opacityOnHover":0}});</script></body>
</html>
